import os
import numpy as np
import torch
import random
from env.environment import Environment
from utils.utils import TrainingLogger
from args import get_args,get_test_args
import json

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def set_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def evaluate_model(model, env, n_episodes=5):
    """ è¯„ä¼°æ¨¡å‹åœ¨ç¯å¢ƒä¸­çš„è¡¨ç° """
    total_reward = 0
    for _ in range(n_episodes):
        state = env.reset()
        done = False
        while not done:
            action = model.select_action(state)  # ğŸ”¥ ç¡®ä¿æ¨¡å‹æœ‰ select_action æ–¹æ³•
            state, reward, done, _ = env.step(action)
            total_reward += reward
    return total_reward / n_episodes  # è®¡ç®—å¹³å‡å¥–åŠ±


def train_model(env_name, algo, seed, expert_data):
    """ è®­ç»ƒ GAIL / BC / DAgger """
    from student.BC import student_BC
    from student.GAIL import GAIL
    from student.DAgger import DAgger

    set_seed(seed)
    env = Environment(env_type=env_name)

    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    max_action = float(env.action_space.high[0])

    # é€‰æ‹©æ¨¡å‹
    if algo == "BC":
        model = student_BC(state_dim, action_dim, max_action)
    elif algo == "GAIL":
        model = GAIL(state_dim, action_dim, max_action)
    elif algo == "DAgger":
        model = DAgger(state_dim, action_dim, max_action)
    else:
        raise ValueError(f"æœªçŸ¥ç®—æ³•: {algo}")

    logger = TrainingLogger(args)

    # è®­ç»ƒè½®æ•°
    max_episodes = 1000
    reward_list = []
    for episode in range(max_episodes):
        episode_reward = 0
        states, actions = expert_data
        if algo == "BC" :
            loss_info = model.learn(states, actions)  # BC & DAgger è®­ç»ƒæ–¹æ³•æ˜¯ learn()
        elif algo == "DAgger":
            loss_info = model.train(states, actions)
        elif algo == "GAIL":
            # model.clear_trajectory()
            state = env.reset()
            for step in range(200):
                action = model.select_action(state)
                next_state, reward, done, _ = env.step(action)
                episode_reward += reward
                model.store_transition(state, action)
                state = next_state
                if done:
                    break

            d_loss, p_loss = model.train(expert_data[0], expert_data[1], env)
            # print(d_loss,p_loss)

        # è¯„ä¼°å½“å‰æ¨¡å‹
        if (episode + 1) % 10 == 0:
            reward = evaluate_model(model, env)
            reward_list.append(reward)
            # model.save_model(env_name, episode, reward)
            logger.log_episode(reward, episode + 1)
            print(f"[{env_name} - {algo} - Seed {seed}] Episode {episode + 1}, Reward: {reward:.2f}")
            # åˆ›å»ºå­—å…¸


    # ä¿å­˜æ¨¡å‹
    model_dir = f"models/{env_name}/{algo}/seed_{2023}"
    os.makedirs(model_dir, exist_ok=True)
    reward_dict = {'rewards': reward_list}
    # dumps å°†æ•°æ®è½¬æ¢æˆå­—ç¬¦ä¸²
    info_json = json.dumps(reward_dict)
    # æ˜¾ç¤ºæ•°æ®ç±»å‹
    f = open(model_dir+'_rewards.json', 'w')
    f.write(info_json)
    model.save(f"{model_dir}/{algo}_final.pth")

    logger.save_all()
    return model


if __name__ == "__main__":
    args = get_args()
    test_args = get_test_args()
    env_list = ["env5"]
    # algos = [ "BC"]
    algos = ["GAIL",]
    seeds = [22]

    for env_name in env_list:
        args.env_type = env_name
        test_args.env_type = env_name
        for algo in algos:
            for seed in seeds:
                # åŠ è½½ä¸“å®¶æ•°æ®
                # expert_data_path = f"expert_data/{env_name}_astar.npy"
                expert_data_path = f"expert_data/{env_name}_td3.npy"
                if not os.path.exists(expert_data_path):
                    print(f"âŒ ä¸“å®¶æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {expert_data_path}, è·³è¿‡è®­ç»ƒï¼")
                    continue

                expert_data = np.load(expert_data_path, allow_pickle=True).item()

                states = np.array(expert_data["states"])
                actions = np.array(expert_data["actions"])

                train_model(env_name, algo, seed, (states, actions))
